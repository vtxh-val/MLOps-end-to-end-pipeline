MLOps Pipeline — DVC & MLflow (Apple AAPL example)
==================================================

This repository demonstrates an end-to-end MLOps pipeline using **DVC** for data/version management and **MLflow** (tracked via DagsHub) for experiment tracking. The example uses `yfinance` (AAPL) as the data source and trains XGBoost models for next-day _return_ prediction (regression) and _trend_ classification (up/down).

Project overview
----------------

Key stages (each a separate Python module and DVC stage):

*   `data_ingestion.py` — download AAPL via `yfinance`, save raw artifacts to `data/raw/`.
*   `data_preprocessing.py` — clean & flatten CSV headers, save cleaned CSV to `data/processed/cleaned.csv`.
*   `feature_engineering.py` — create lags, returns, rolling stats, RSI, targets; output `data/features/engineered.csv`.
*   `model_building.py` — time-based split, train XGBoost regression & classifier, save models to `models/` and train/test CSVs to `data/input/`. Also logs parameters to MLflow when run as the experiment starter.
*   `model_evaluation.py` — loads pickled models + pre-saved test sets from `data/input/`, generates plots and metrics, and logs metrics to MLflow (DagsHub).


Prerequisites
-------------

*   Install packages: `pip install -r requirements.txt` (include: yfinance, mlflow, dvc, dagshub)
*   DagsHub account (if you want remote MLflow tracking)
*   Git & DVC initialized in repo

⚡ Quick Setup
-------------

1.  **Clone the repository:**

    git clone https://github.com/vtxh-val/MLOps-end-to-end-pipeline.git
    cd MLOps-end-to-end-pipeline

2.  **Set up environment variables:**

    Create a `.env` file in the project root with the following variables:

    REPO_OWNER=your-dagshub-username
    REPO_NAME=MLOps-end-to-end-pipeline
    TRACKING_URL=https://dagshub.com/<your-username>/MLOps-end-to-end-pipeline.mlflow

3.  **Load environment variables in your script:**

    from dotenv import load_dotenv
    import os
    
    load_dotenv()
    
    repo_owner = os.getenv("REPO_OWNER")
    repo_name = os.getenv("REPO_NAME")
    tracking_url = os.getenv("TRACKING_URL")
    

Run the pipeline (DVC)
----------------------

Example `dvc.yaml` defines stages that call the Python scripts. To run the pipeline end-to-end locally:

    # run all stages
    dvc repro
    
    # or run a single stage (e.g., feature engineering)
    dvc repro feature_engineering
        

Best practices & tips
---------------------

*   Always **drop target columns** before saving feature matrices to avoid leakage (e.g., drop `Target_Return` and `Target_Trend` from `X`).
*   Use `os.makedirs(os.path.dirname(path), exist_ok=True)` to avoid creating a folder with a filename by mistake.


Next steps / Extension ideas
----------------------------

*   Add cross-validation and hyperparameter search (optuna / scikit-optimize) and log trials to MLflow.
*   Use DVC remotes (S3/GDrive) for large data and model artifacts.
*   Containerize stages and use GitHub Actions to run `dvc repro` in CI for reproducible pipelines.
*   Experiment with different targets (multi-day horizon, volatility forecasting) and advanced models (LSTM, Transformer, TabNet).

License
-------

MIT License — adapt freely for your internal projects. Created for demonstration purposes.

Generated by an assistant — adapt the commands and paths to your environment.
